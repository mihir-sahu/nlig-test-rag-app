"""
Vector retriever implementation
Handles similarity search and context retrieval
"""
import logging
from typing import List, Dict, Any

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

from app.config import settings
from app.ingest import get_vector_store

logger = logging.getLogger(__name__)


class RAGRetriever:
    """Retriever for RAG system"""
    
    def __init__(self):
        self.vector_store = get_vector_store()
        self.llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            temperature=settings.TEMPERATURE,
            openai_api_key=settings.OPENAI_API_KEY
        ) if settings.OPENAI_API_KEY else None
    
    async def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """
        Retrieve relevant documents for a query
        
        Args:
            query: User query
            top_k: Number of documents to retrieve
        
        Returns:
            List of relevant documents with metadata
        """
        if not self.vector_store:
            raise ValueError("Vector store not initialized")
        
        k = top_k or settings.TOP_K
        
        # Similarity search
        results = self.vector_store.similarity_search_with_score(
            query=query,
            k=k
        )
        
        documents = []
        for doc, score in results:
            documents.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": float(score)
            })
        
        logger.info(f"Retrieved {len(documents)} documents for query")
        
        return documents
    
    async def retrieve_with_compression(
        self,
        query: str,
        top_k: int = None
    ) -> List[Dict[str, Any]]:
        """
        Retrieve with contextual compression
        
        Uses LLM to extract only relevant parts of retrieved documents
        """
        if not self.vector_store or not self.llm:
            raise ValueError("Vector store and LLM required for compression")
        
        k = top_k or settings.TOP_K
        
        # Base retriever
        base_retriever = self.vector_store.as_retriever(search_kwargs={"k": k})
        
        # Compression retriever
        compressor = LLMChainExtractor.from_llm(self.llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        
        # Retrieve compressed documents
        compressed_docs = compression_retriever.get_relevant_documents(query)
        
        documents = []
        for doc in compressed_docs:
            documents.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })
        
        logger.info(f"Retrieved {len(documents)} compressed documents")
        
        return documents


# Global retriever instance
retriever = RAGRetriever()
